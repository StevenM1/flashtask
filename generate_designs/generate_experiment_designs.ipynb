{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "from pprint import pprint\n",
    "from glob import glob\n",
    "import cPickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to generate blocks of trials\n",
    "Localizer, cognitive, and limbic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_localizer_block_single_effector(n_trials, response_modality='eye', \n",
    "                                           block_number=None, pseudorandomize=True, \n",
    "                                           add_timing=True, null_trials=0, TR=2):\n",
    "    \n",
    "    # Only two trial types here: 'left' is correct, or 'right' is correct\n",
    "    trial_types = np.repeat([0, 1], repeats=n_trials/2)  # left/right\n",
    "    \n",
    "    # Initialize arrays\n",
    "    cue_by_trial = np.zeros(n_trials, dtype='<U5')\n",
    "    correct_answers = np.zeros(n_trials, dtype=np.int8)\n",
    "\n",
    "    # Define the cues for every trial\n",
    "    cue_by_trial[(trial_types == 0)] = 'LEFT'\n",
    "    cue_by_trial[(trial_types == 1)] = 'RIGHT'\n",
    "#     cue_by_trial[(trial_types == 2) | (trial_types == 3)] = 'anti'\n",
    "\n",
    "    # Define the responses ('correct answers')/directions for every trial\n",
    "    correct_answers[trial_types == 0] = 0\n",
    "    correct_answers[trial_types == 1] = 1\n",
    "#     correct_answers[(trial_types == 0) | (trial_types == 2)] = 0  # 0 = respond LEFT\n",
    "#     correct_answers[(trial_types == 1) | (trial_types == 3)] = 1  # 1 = respond RIGHT\n",
    "    \n",
    "    # Create dataframe for easier handling\n",
    "    trial_data = pd.DataFrame({'correct_answer': correct_answers,\n",
    "                               'cue': cue_by_trial,\n",
    "                               'trial_type': trial_types})\n",
    "    \n",
    "    # Should we pseudorandomize?\n",
    "    if pseudorandomize:\n",
    "        trial_data = Pseudorandomizer(trial_data, max_identical_iters={'correct_answer': 3,\n",
    "                                                                       'cue': 3}).run()\n",
    "    \n",
    "    trial_data['null_trial'] = False\n",
    "    if null_trials > 0:\n",
    "        trial_data = add_pseudorandom_null_trials(trial_data, n_null_trials=null_trials)\n",
    "    \n",
    "    # Add block number for completeness\n",
    "    if block_number is not None:\n",
    "        trial_data['block'] = block_number\n",
    "        trial_data['block_type'] = 'localizer'\n",
    "\n",
    "    # Usually, we also want to add the duration of all the 'trial phases'\n",
    "    if add_timing:\n",
    "        # Set phase_3 and phase_0 to 0 (no post-cue fixcross, no feedback)\n",
    "        trial_data = get_localizer_timing(trial_data, TR=TR)\n",
    "    \n",
    "    trial_data['response_modality'] = response_modality.lower()\n",
    "    \n",
    "    return trial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_cognitive_block(n_trials, block_number=None, response_modality=None, \n",
    "                           add_timing=True, pseudorandomize=True, n_null_trials=0, TR=2):\n",
    "    \"\"\"\n",
    "    Creates a block of SAT-trials; mixing speed and accuracy trials\n",
    "    \"\"\"\n",
    "    \n",
    "    trial_types = np.hstack((np.repeat([0, 1], repeats=n_trials / 4),   # SPEED cue, left/right corr\n",
    "                             np.repeat([2, 3], repeats=n_trials / 4)))  # ACCURACY cue, left/right corr\n",
    "\n",
    "    if trial_types.shape[0] != n_trials:\n",
    "        raise(ValueError('The provided n_trials (%d) could not be split into the correct number of trial types. '\n",
    "                         'Closest option is %d trials' % (n_trials, trial_types.shape[0])))\n",
    "\n",
    "    cue_by_trial = np.zeros(n_trials, dtype='<U5')\n",
    "    correct_answers = np.zeros(n_trials, dtype=np.int8)\n",
    "\n",
    "    cue_by_trial[(trial_types == 0) | (trial_types == 1)] = 'SPD'\n",
    "    cue_by_trial[(trial_types == 2) | (trial_types == 3)] = 'ACC'\n",
    "\n",
    "    correct_answers[(trial_types == 0) | (trial_types == 2)] = 0  # 0 = left is correct\n",
    "    correct_answers[(trial_types == 1) | (trial_types == 3)] = 1  # 1 = right is correct\n",
    "\n",
    "    # Create dataframe for easier handling\n",
    "    trial_data = pd.DataFrame({'correct_answer': correct_answers,\n",
    "                               'cue': cue_by_trial,\n",
    "                               'trial_type': trial_types})\n",
    "    \n",
    "    if pseudorandomize:\n",
    "        trial_data = Pseudorandomizer(trial_data, \n",
    "                                      max_identical_iters={'cue': 5, 'correct_answer': 5}).run()\n",
    "        \n",
    "    if n_null_trials > 0:\n",
    "        trial_data['null_trial'] = False\n",
    "        trial_data = add_pseudorandom_null_trials(trial_data, \n",
    "                                                  n_null_trials=n_null_trials, \n",
    "                                                  null_column_name='null_trial')\n",
    "\n",
    "    if block_number is not None:\n",
    "        trial_data['block'] = block_number\n",
    "        \n",
    "    if response_modality is not None:\n",
    "        trial_data['response_modality'] = response_modality\n",
    "        trial_data['block_type'] = 'cognitive_%s' % response_modality \n",
    "\n",
    "    if add_timing:\n",
    "        while True:\n",
    "            trial_data = get_block_timing(trial_data, TR=TR)  # Add default timing\n",
    "            \n",
    "            if check_good_ITI_phase0(trial_data):\n",
    "                break\n",
    "        \n",
    "    return trial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_limbic_block(n_trials, subject_number=1, block_number=None, \n",
    "                        response_modality=None, add_timing=True, pseudorandomize=True,\n",
    "                        n_null_trials=0, TR=2):\n",
    "    \n",
    "    trial_types = np.hstack((np.repeat([0, 1], repeats=n_trials/6),    # Neutral cue, left/right corr\n",
    "                             np.repeat([2, 3], repeats=n_trials/6),    # Left cue, left/right corr\n",
    "                             np.repeat([4, 5], repeats=n_trials/6)))   # Right cue, left/right corr\n",
    "\n",
    "    if trial_types.shape[0] != n_trials:\n",
    "        raise(ValueError('The provided n_trials (%d) could not be split into the correct number of trial types. '\n",
    "                     'Closest option is %d trials' % (n_trials, trial_types.shape[0])))\n",
    "\n",
    "    cue_by_trial = np.zeros(n_trials, dtype='<U5')\n",
    "    correct_answers = np.zeros(n_trials, dtype=np.int8)\n",
    "\n",
    "    cue_by_trial[(trial_types == 0) | (trial_types == 1)] = 'NEU'\n",
    "    cue_by_trial[(trial_types == 2) | (trial_types == 3)] = 'LEFT'\n",
    "    cue_by_trial[(trial_types == 4) | (trial_types == 5)] = 'RIGHT'\n",
    "\n",
    "    correct_answers[(trial_types == 0) |\n",
    "                    (trial_types == 2) |\n",
    "                    (trial_types == 4)] = 0  # 0 = left is correct\n",
    "    correct_answers[(trial_types == 1) |\n",
    "                    (trial_types == 3) |\n",
    "                    (trial_types == 5)] = 1  # 1 = right is correct\n",
    "\n",
    "    # Create dataframe for easier handling\n",
    "    trial_data = pd.DataFrame({'correct_answer': correct_answers,\n",
    "                               'cue': cue_by_trial,\n",
    "                               'trial_type': trial_types})\n",
    "\n",
    "    if pseudorandomize:\n",
    "        trial_data = Pseudorandomizer(trial_data, \n",
    "                                      max_identical_iters={'cue': 4, 'correct_answer': 4}).run()\n",
    "    \n",
    "    if n_null_trials > 0:\n",
    "        trial_data['null_trial'] = False\n",
    "        trial_data = add_pseudorandom_null_trials(trial_data, \n",
    "                                                  n_null_trials=n_null_trials, \n",
    "                                                  null_column_name='null_trial')\n",
    "    \n",
    "    if block_number is not None:\n",
    "        trial_data['block'] = block_number\n",
    "        \n",
    "    if response_modality is not None:\n",
    "        trial_data['response_modality'] = response_modality\n",
    "        trial_data['block_type'] = 'limbic_%s' % response_modality \n",
    "\n",
    "    if add_timing:\n",
    "        while True:\n",
    "            trial_data = get_block_timing(trial_data, TR=TR)  # Add default timing\n",
    "            \n",
    "            if check_good_ITI_phase0(trial_data):\n",
    "                break\n",
    "\n",
    "    return trial_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that creates timing columns for a block of trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_localizer_timing(trial_data, phase_0=None, phase_1=None, phase_2=None, phase_3=None, phase_4=None, phase_5=None, phase_6=None, TR=2):\n",
    "    \"\"\"\n",
    "    Each localizer trial consists of 7 phases.\n",
    "    \n",
    "    In phase_0, we wait for the scanner pulse. Note that phase_0 of trial n is the ITI after trial n-1. Set this timing always to 0: it is the `minimum` time to wait for the pulse\n",
    "    In phase_1, we show the pre-cue fixation cross. By default, timing is jittered (0s, 0.5s, 1s, 1.5s if TR=2 -- 0, .75, 1.5, 2.25 if TR=3).\n",
    "    In phase_2, we show the cue. Follows an exponential distrbibu.\n",
    "    In phase_3, we show the post-cue fixation cross. Defaults to 0s.\n",
    "    In phase_4, we assume the participant responds, and wait a bit until we show the fix cross. Defaults to 0.6s\n",
    "    In phase_5 and phase_6, we do nothing (exist for compatibility with the experimental blocks)\n",
    "    Phase_7 is ITI\n",
    "    \"\"\"\n",
    "    \n",
    "    if TR == 2:\n",
    "        trial_data['phase_0'] = 0 if phase_0 is None else phase_0\n",
    "        trial_data['phase_1'] = np.random.choice([0.2, .7, 1.2, 1.7], size=trial_data.shape[0]) if phase_1 is None else phase_1\n",
    "        trial_data['phase_2'] = 0.8 if phase_2 is None else phase_2\n",
    "        trial_data['phase_3'] = 0 if phase_3 is None else phase_3\n",
    "        trial_data['phase_4'] = 0.6 if phase_4 is None else phase_4\n",
    "        trial_data['phase_5'] = 0 if phase_5 is None else phase_5\n",
    "        trial_data['phase_6'] = 0 if phase_6 is None else phase_6\n",
    "    elif TR == 3:\n",
    "        trial_data['phase_0'] = 0 if phase_0 is None else phase_0\n",
    "        trial_data['phase_1'] = np.random.choice([0, .750, 1.500, 2.250], size=trial_data.shape[0]) if phase_1 is None else phase_1\n",
    "        trial_data['phase_2'] = np.round(np.random.exponential(scale=1/6, size=trial_data.shape[0])+.8, 3) if phase_2 is None else phase_2\n",
    "#        trial_data['phase_2'] = np.round(np.random.uniform()  ) if phase_2 is None else phase_2\n",
    "        trial_data['phase_3'] = 0 if phase_3 is None else phase_3\n",
    "        trial_data['phase_4'] = 0.8 if phase_4 is None else phase_4\n",
    "        trial_data['phase_5'] = 0 if phase_5 is None else phase_5\n",
    "        trial_data['phase_6'] = 0 if phase_6 is None else phase_6\n",
    "\n",
    "    # Calculate duration of trial (depends on random, jittered durations of the fix cross)\n",
    "    trial_data['trial_duration'] = trial_data[['phase_' + str(x) for x in range(7)]].sum(axis=1)\n",
    "\n",
    "    # Because of TR = 2s, some trials can last 8 seconds, but most will last 10. Find trials with total time < 8 seconds\n",
    "    # We calculate the ITI as the difference between the minimum number of pulses necessary for all phases to show.\n",
    "    # [same applies to TR = 3]\n",
    "#     min_TRs = np.ceil(trial_data['trial_duration'].values / TR)\n",
    "#     trial_data['phase_7'] = min_TRs*TR - trial_data['trial_duration'].values\n",
    "    trial_data['phase_7'] = 6 - trial_data['trial_duration'].values\n",
    "\n",
    "    # Recalculate trial duration so it includes the ITI\n",
    "    trial_data['trial_duration'] = trial_data[['phase_' + str(x) for x in range(8)]].sum(axis=1)\n",
    "\n",
    "    # Add trial start times relative to start of block\n",
    "    trial_data['trial_start_time_block'] = trial_data['trial_duration'].shift(1).cumsum()\n",
    "    trial_data.loc[0, 'trial_start_time_block'] = 0\n",
    "\n",
    "    # Add cue onset times relative to start of block\n",
    "    trial_data['cue_onset_time_block'] = trial_data['trial_start_time_block'] + \\\n",
    "                                         trial_data['phase_1']\n",
    "\n",
    "    # Add stimulus onset times relative to start of block\n",
    "    trial_data['stimulus_onset_time_block'] = trial_data['trial_start_time_block'] + \\\n",
    "                                              trial_data['phase_1'] + \\\n",
    "                                              trial_data['phase_2'] + \\\n",
    "                                              trial_data['phase_3']\n",
    "    return trial_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_block_timing(trial_data, phase_0=None, phase_1=None, phase_2=None, phase_3=None, phase_4=None, phase_5=None, phase_6=None, TR=2):\n",
    "    \"\"\"\n",
    "    Each trial consists of 7 phases.\n",
    "    \n",
    "    In phase_0, we wait for the scanner pulse. Note that phase_0 of trial n is the ITI after trial n-1. Set this timing always to 0: it is the `minimum` time to wait for the pulse\n",
    "    In phase_1, we show the pre-cue fixation cross. By default, timing is jittered (0s, 0.5s, 1s, 1.5s)\n",
    "    In phase_2, we show the cue. In decision-making trials, this is 4.8 seconds.\n",
    "    In phase_3, we show the post-cue fixation cross. Timing is jittered (0s, 0.5s, 1s, 1.5s)\n",
    "    In phase_4, we show the stimulus. Default is 1.5s.\n",
    "    Phase 5 is defined as the period of stimulus presentation, after the participant made a response. The duration is determined by the participant RT, so not set here.\n",
    "    In phase_6, we show feedback. Default is 0.35s.    \n",
    "    \"\"\"\n",
    "    \n",
    "    if TR == 2:\n",
    "        trial_data['phase_0'] = 0 if phase_0 is None else phase_0\n",
    "        trial_data['phase_1'] = np.random.choice([0, .5, 1, 1.5], size=trial_data.shape[0]) if phase_1 is None else phase_1\n",
    "        trial_data['phase_2'] = 4.8 if phase_2 is None else phase_2\n",
    "        trial_data['phase_3'] = np.random.choice([0, .5, 1, 1.5], size=trial_data.shape[0]) if phase_3 is None else phase_3\n",
    "        trial_data['phase_4'] = 2 if phase_4 is None else phase_4\n",
    "        trial_data['phase_5'] = 0 if phase_5 is None else phase_5\n",
    "        trial_data['phase_6'] = 0.35 if phase_6 is None else phase_6\n",
    "    elif TR == 3:\n",
    "        trial_data['phase_0'] = 0 if phase_0 is None else phase_0\n",
    "        trial_data['phase_1'] = np.random.choice([0, .750, 1.500, 2.250], size=trial_data.shape[0]) if phase_1 is None else phase_1\n",
    "        trial_data['phase_2'] = 1 if phase_2 is None else phase_2\n",
    "        trial_data['phase_3'] = np.random.choice([0.750, 1.500, 2.250, 3.000], size=trial_data.shape[0]) if phase_3 is None else phase_3\n",
    "        trial_data['phase_4'] = 2 if phase_4 is None else phase_4\n",
    "        trial_data['phase_5'] = 0 if phase_5 is None else phase_5\n",
    "        trial_data['phase_6'] = 0.5 if phase_6 is None else phase_6\n",
    "\n",
    "\n",
    "    # Calculate duration of trial (depends on random, jittered durations of the fix cross)\n",
    "    trial_data['trial_duration'] = trial_data[['phase_' + str(x) for x in range(7)]].sum(axis=1)\n",
    "\n",
    "    if TR == 2:\n",
    "        # Because of TR = 2s, some trials can last 8 seconds, but most will last 10. Find trials with total time < 8 seconds\n",
    "        # We calculate the ITI as the difference between the minimum number of pulses necessary for all phases to show.\n",
    "        min_TRs = np.ceil(trial_data['trial_duration'].values / TR)\n",
    "        trial_data['phase_7'] = min_TRs*TR - trial_data['trial_duration'].values\n",
    "    elif TR == 3:\n",
    "        # In this case, fill all trials until 9s have passed.\n",
    "        trial_data['phase_7'] = 9 - trial_data['trial_duration'].values\n",
    "\n",
    "    # Recalculate trial duration so it includes the ITI\n",
    "    trial_data['trial_duration'] = trial_data[['phase_' + str(x) for x in range(8)]].sum(axis=1)\n",
    "\n",
    "    # Add trial start times relative to start of block\n",
    "    trial_data['trial_start_time_block'] = trial_data['trial_duration'].shift(1).cumsum()\n",
    "    trial_data.loc[0, 'trial_start_time_block'] = 0\n",
    "\n",
    "    # Add cue onset times relative to start of block\n",
    "    trial_data['cue_onset_time_block'] = trial_data['trial_start_time_block'] + \\\n",
    "                                         trial_data['phase_1']\n",
    "\n",
    "    # Add stimulus onset times relative to start of block\n",
    "    trial_data['stimulus_onset_time_block'] = trial_data['trial_start_time_block'] + \\\n",
    "                                              trial_data['phase_1'] + \\\n",
    "                                              trial_data['phase_2'] + \\\n",
    "                                              trial_data['phase_3']\n",
    "    return trial_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to check timing\n",
    "If ITI after trial n is 0, it is not allowed to have trial n+1 phase1 = 0 (otherwise, a new cue can be shown immediately after feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_good_ITI_phase0(data):\n",
    "    # Get rid of Null Trials, and the trials before the Null Trials\n",
    "    nulls = data[data['null_trial'] == True].index.values\n",
    "    nulls = np.hstack((nulls, data.shape[0]-1))\n",
    "    \n",
    "    start_id = 0\n",
    "    for end_id in nulls:\n",
    "        data_subset = data.iloc[np.arange(start_id, end_id)].copy()\n",
    "        # Shift rows in column phase_1\n",
    "        data_subset['phase_1'] = data_subset['phase_1'].shift(-1)\n",
    "\n",
    "        # Check whether (shifted) phase_1 values == phase_7 values, AND phase_1 values is 0.\n",
    "        idx = (data_subset['phase_1'].values == data_subset['phase_7'].values) & (data_subset['phase_1'].values == 0.00)\n",
    "\n",
    "        if np.sum(idx) > 0:\n",
    "#            print(data_subset[['phase_1', 'phase_7']])\n",
    "            return False\n",
    "        \n",
    "        start_id = end_id + 1\n",
    "        \n",
    "    return True  \n",
    "    \n",
    "# dat = create_cognitive_block(n_trials=100,\n",
    "#                         block_number=1, \n",
    "#                         response_modality='hand',\n",
    "#                         n_null_trials=7, \n",
    "#                         TR=3)\n",
    "# print(dat[['phase_1', 'phase_7']].head(6))\n",
    "# check_good_ITI_phase0(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for pseudorandomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Pseudorandomizer(object):\n",
    "    \n",
    "    def __init__(self, data, max_identical_iters={'cue': 4, 'correct_answer': 4}):  \n",
    "        self.data = data\n",
    "        self.max_identical_iters = {x: y+1 for x, y in max_identical_iters.items()}\n",
    "                                    # add 1: if 4 rows is allowed, only give an error after 5 identical rows\n",
    "    \n",
    "    def check_trial_rows(self, data, row_n): \n",
    "        \"\"\"\n",
    "        Returns True if any of the conditions for pseudorandomization are violated for the given rows, \n",
    "        False if they are fine.\n",
    "        \"\"\"\n",
    "        \n",
    "        # First, check for the maximum iterations\n",
    "        for column, max_iter in self.max_identical_iters.items():\n",
    "            if row_n - max_iter < 0:\n",
    "                continue\n",
    "\n",
    "            # Select rows [max_iter-1 - row_n] we're going to check. Never select any row with index < 0\n",
    "            row_selection = [x for x in np.arange(row_n, row_n-max_iter, -1)]\n",
    "\n",
    "            # Next, we check if the selected rows only contain *1* trial type. \n",
    "            # If so, this means we have max_iter rows of the same trials, and we need to change something.\n",
    "            if data.iloc[row_selection][column].nunique() == 1:\n",
    "                return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def run(self, debug=False):\n",
    "        \"\"\"\n",
    "        Pseudorandomizes: makes sure that it is not possible to have more than x iterations for every type of column, specified in columns.\n",
    "        \"\"\"\n",
    "        # Start by copying from original data, and shuffle\n",
    "        self.data = self.data.sample(frac=1, \n",
    "                                     random_state=np.random.randint(0, 1e7, dtype='int')).reset_index(drop=True) \n",
    "        \n",
    "        if debug:\n",
    "            outer_while_i = 0\n",
    "            debug_print_after_i = 100\n",
    "\n",
    "        good_set = False\n",
    "        while not good_set:\n",
    "            if debug:\n",
    "                outer_while_i += 1\n",
    "\n",
    "            reshuffle = False  # Assume the dataset does not need reshuffling.\n",
    "            for row_n in range(0, self.data.shape[0]):\n",
    "\n",
    "                # Select rows [max_iter-1 - row_n] we're going to check\n",
    "\n",
    "                # Check if the current row, and the (max_iters-1) rows before, are the same value (number of unique values = 1).\n",
    "                # If so, then move the current row number to the bottom of the dataframe. However, we need to re-check the same four rows again\n",
    "                # after moving a row to the bottom: therefore, a while loop is necessary.\n",
    "                checked_row = False\n",
    "                n_attempts_at_moving = 0\n",
    "                \n",
    "                if debug:\n",
    "                    inner_while_i = 0\n",
    "                \n",
    "                while not checked_row:\n",
    "                    if debug:\n",
    "                        inner_while_i += 1\n",
    "                        if inner_while_i > debug_print_after_i:\n",
    "                            print('New inner loop started for current row')\n",
    "\n",
    "                    if self.check_trial_rows(self.data, row_n):\n",
    "                        if debug and inner_while_i > debug_print_after_i:\n",
    "                            print('Found too many consecutively identical rows.')\n",
    "\n",
    "                        # If there are too many consecutively identical rows at the bottom of the dataframe, \n",
    "                        # break and start over/shuffle\n",
    "                        if row_n >= (self.data.shape[0] - self.max_identical_iters[self.max_identical_iters.keys()[0]]):\n",
    "                            if debug and inner_while_i > debug_print_after_i:\n",
    "                                print('These occurred at row_n %d, which is at the bottom of the DF.' % row_n)\n",
    "\n",
    "                            checked_row = True\n",
    "                            reshuffle = True\n",
    "\n",
    "                        # Too many consecutive identical rows? Move row_n to the bottom, and check again with the new row_n.\n",
    "                        else:\n",
    "                            if debug and inner_while_i > debug_print_after_i:\n",
    "                                print('These occurred at row_n %d. Checking the remainder of the DF.' % row_n)\n",
    "\n",
    "                            # Check if moving to the bottom even makes sense: if all remaining values are identical, it doesn't.\n",
    "                            if (self.data.iloc[row_n:][self.max_identical_iters.keys()].nunique().values < 2).any():\n",
    "                                if debug and inner_while_i > debug_print_after_i:\n",
    "                                    print('All remaining values are identical. I should stop the for-loop, and start over.')\n",
    "\n",
    "                                checked_row = True\n",
    "                                reshuffle = True\n",
    "                            else:\n",
    "                                if n_attempts_at_moving < 50:\n",
    "                                    n_attempts_at_moving += 1\n",
    "\n",
    "                                    if debug and inner_while_i > debug_print_after_i:\n",
    "                                        print('Not all remaining values are identical. I should move the final part to the bottom.')\n",
    "\n",
    "                                    # If not, move the current row to the bottom\n",
    "                                    row_to_move = self.data.iloc[row_n,:]\n",
    "\n",
    "                                    # Delete row from df\n",
    "                                    self.data.drop(row_n, axis=0, inplace=True)\n",
    "\n",
    "                                    # Append original row to end. Make sure to reset index\n",
    "                                    self.data = self.data.append(row_to_move).reset_index(drop=True)\n",
    "\n",
    "                                # If we already tried moving the current row to the bottom for 50 times, let's forget about it and restart\n",
    "                                else:\n",
    "                                    checked_row = True\n",
    "                                    reshuffle = True\n",
    "                    else:\n",
    "                        if debug and inner_while_i > debug_print_after_i:\n",
    "                            print('Checked row, but the row is fine. Next row.')\n",
    "                        checked_row = True\n",
    "\n",
    "                if reshuffle:\n",
    "                    good_set = False\n",
    "                    break  # out of the for loop\n",
    "\n",
    "                # Reached the bottom of the dataframe, but no reshuffle call? Then we're set.\n",
    "                if row_n == self.data.shape[0]-1:\n",
    "                    good_set = True\n",
    "\n",
    "            if reshuffle:\n",
    "                # Shuffle, reset index to ensure trial_data.drop(row_n) rows\n",
    "                self.data = self.data.sample(frac=1, random_state=np.random.randint(0, 1e7, dtype='int')).reset_index(drop=True)\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "def add_pseudorandom_null_trials(data, min_row=4, max_row=4, min_n_rows_separate=7, \n",
    "                                n_null_trials=10, null_column_name=''):\n",
    "    \"\"\" \n",
    "    Adds null trials interspersed at pseudorandom locations. You can determine the minimum\n",
    "    number of trials at the start before a null trial, the minimum number of trials at the end in which no\n",
    "    nulls are shown, and the minimum number of trials that the null trials have to be separated \n",
    "    \"\"\"\n",
    "    \n",
    "    good_idx = False\n",
    "    while not good_idx:\n",
    "        indx = np.random.choice(np.arange(min_row, data.shape[0]-max_row), \n",
    "                                replace=False, size=n_null_trials)\n",
    "        diffs = np.diff(np.sort(indx))\n",
    "        if (diffs >= min_n_rows_separate).all():\n",
    "            good_idx = True\n",
    "    \n",
    "    data.index = np.setdiff1d(np.arange(data.shape[0] + n_null_trials), indx)\n",
    "    new_rows = pd.DataFrame({null_column_name: [True]*n_null_trials}, columns=data.columns, index=indx)    \n",
    "    data = data.append(new_rows).sort_index()\n",
    "    \n",
    "    # Always end with a null trial\n",
    "    last_row = pd.DataFrame({null_column_name: [True]*1}, columns=data.columns, index=[data.shape[0]])\n",
    "    data = data.append(last_row).sort_index() \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for brute-force optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_localizer(n_localizer_blocks, n_trials_per_localizer_block, localizer_order, block_number=0, pseudorandomize=True, TR=3):\n",
    "\n",
    "    # Initiate empty dataframe, generate block\n",
    "    block_data = pd.DataFrame()\n",
    "    for localizer_block in range(int(n_localizer_blocks/2)):\n",
    "        loc_block = create_localizer_block_single_effector(n_trials=n_trials_per_localizer_block, \n",
    "                                            response_modality=localizer_order[0],\n",
    "                                            block_number=block_number,\n",
    "                                            pseudorandomize=pseudorandomize, TR=TR)\n",
    "        block_data = block_data.append(loc_block)\n",
    "        loc_block = create_localizer_block_single_effector(n_trials=n_trials_per_localizer_block, \n",
    "                                            response_modality=localizer_order[1],\n",
    "                                            block_number=block_number,\n",
    "                                            pseudorandomize=pseudorandomize, TR=TR)\n",
    "        block_data = block_data.append(loc_block)\n",
    "\n",
    "    return block_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to convolve design\n",
    "from nipy.modalities.fmri import hrf, utils\n",
    "\n",
    "def stim_to_design(pp_design, block=None, silent=False):\n",
    "    \n",
    "    # Check if we only need to do a subset of the design\n",
    "    if block is not None:\n",
    "        pp_design = pp_design.loc[pp_design['block'] == block]\n",
    "    \n",
    "    # Get rid of null trials\n",
    "    pp_design = pp_design.loc[pp_design['null_trial'] == False,:]\n",
    "\n",
    "    # hrf.glover is a symbolic function; get a function of time to work on arrays\n",
    "    hrf_func = utils.lambdify_t(hrf.glover(utils.T))\n",
    "    \n",
    "    max_time = np.ceil((pp_design['stimulus_onset_time'].max()+25)*10)\n",
    "\n",
    "    if 0 in pp_design['block'].unique():\n",
    "        block0_trials = pp_design.loc[pp_design['block'] == 0]\n",
    "        # Get cue-types and response types for the first block\n",
    "        loc_cue_vec = np.zeros(shape=(int(max_time), 4))\n",
    "        response_vec = np.zeros(shape=(int(max_time), 4))\n",
    "        loc_cue_names = []\n",
    "        response_names = []\n",
    "\n",
    "        i = -1\n",
    "        for effector_type in block0_trials['response_modality'].unique():\n",
    "            for cue_type in block0_trials['cue'].unique():\n",
    "                \n",
    "                subset = pp_design.loc[(pp_design['block'] == 0 ) &\n",
    "                                       (pp_design['response_modality'] == effector_type) &\n",
    "                                       (pp_design['cue'] == cue_type)]\n",
    "                i += 1\n",
    "                response_names.append('resp_%s_%s' % (effector_type, cue_type))\n",
    "                loc_cue_names.append('cue_%s_%s' % (effector_type, cue_type))\n",
    "\n",
    "                # Get cue onsets & durations\n",
    "                onsets = np.round(subset['cue_onset_time'].values*10)\n",
    "                durations = np.round(subset['phase_2'].values*10)\n",
    "                for onset, duration in zip(onsets, durations):\n",
    "                    loc_cue_vec[np.arange(onset, onset+duration, dtype='int'), i] = 1\n",
    "\n",
    "                # Get response onsets & durations\n",
    "                onsets = np.round(subset['stimulus_onset_time'].values*10)\n",
    "                durations = np.round(subset['phase_4'].values*10)\n",
    "                for onset, duration in zip(onsets, durations):\n",
    "                    response_vec[np.arange(onset, onset+duration, dtype='int'), i] = 1\n",
    "        \n",
    "        # For all further EVs, make sure not to include the localizer trials.\n",
    "        pp_design = pp_design.loc[pp_design['block'] > 0]\n",
    "\n",
    "    # 10 types of stimuli: (n_cue_types) * (n_stim_types)\n",
    "    stim_vec = np.zeros(shape=(int(max_time), pp_design['correct_answer'].nunique()*pp_design['cue'].nunique()))\n",
    "    stim_names = []\n",
    "\n",
    "    # Get stimulus onsets and durations\n",
    "    i = -1\n",
    "    for stim_type in pp_design['correct_answer'].unique():\n",
    "\n",
    "        for cue_type in pp_design['cue'].unique():\n",
    "            i += 1\n",
    "            stim_names.append('stimulus_' + str(int(stim_type)) + '_' + cue_type)\n",
    "            subset = pp_design.loc[(pp_design['correct_answer'] == stim_type) &\n",
    "                                   (pp_design['cue'] == cue_type)]\n",
    "            stim_onsets = np.round(subset['stimulus_onset_time'].values*10)\n",
    "            stim_durations = np.round(subset['phase_4'].values*10)\n",
    "\n",
    "            for onset, duration in zip(stim_onsets, stim_durations):\n",
    "#                stim_vec = np.hstack((stim_vec, np.zeros(shape=(int(max_time), 1))))\n",
    "                stim_vec[np.arange(onset, onset+duration, dtype='int'), i] = 1\n",
    "\n",
    "    # Get cue onsets by cue type\n",
    "    cue_names = []\n",
    "    n_conditions = len(np.unique(pp_design['cue']))\n",
    "    cue_vec = np.zeros(shape=(int(max_time), n_conditions))  # A column per cue type condition\n",
    "    i = -1\n",
    "    for condition in pp_design['cue'].unique():\n",
    "        i += 1\n",
    "        cue_names.append('cue_' + condition)\n",
    "\n",
    "        # Find cue onsets\n",
    "        onsets = np.round(pp_design.loc[pp_design['cue'] == condition, 'cue_onset_time'].values*10)\n",
    "        durations = np.round(pp_design.loc[pp_design['cue'] == condition, 'phase_2'].values*10)\n",
    "        for onset, duration in zip(onsets, durations):\n",
    "            cue_vec[np.arange(onset, onset+duration, dtype='int'), i] = 1\n",
    "\n",
    "    # Combine everything in a single array\n",
    "    if 'loc_cue_vec' in locals():\n",
    "        ev_vec = np.hstack((loc_cue_vec, response_vec, cue_vec, stim_vec))\n",
    "        ev_names = loc_cue_names + response_names + cue_names + stim_names\n",
    "    else:\n",
    "        ev_vec = np.hstack((cue_vec, stim_vec))\n",
    "        ev_names = cue_names + stim_names\n",
    "\n",
    "    # Create hrf to convolve with\n",
    "    hrf_full = hrf_func(np.linspace(0, stop=int(max_time/10), num=int(max_time)))\n",
    "\n",
    "    # Pre-allocate output. This will be an n_timepoints x n_conditions+1 matrix.\n",
    "    X = np.empty(shape=(int(max_time), ev_vec.shape[1]))\n",
    "\n",
    "    # Convolve everything: the stimulus first, the cues afterwards.\n",
    "    for i, ev_name in enumerate(ev_names):\n",
    "        if not silent:\n",
    "            print('Convolving %s...' % ev_name)\n",
    "        X[:, i] = np.convolve(hrf_full, ev_vec[:, i])[:int(max_time)]\n",
    "            \n",
    "    return X, ev_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_brute_force(n_trials, \n",
    "                         c,   # contrasts to be optimized\n",
    "                         run_type='localizer',\n",
    "                         block_number=0,\n",
    "                         pseudorandomize=True, \n",
    "                         TR=3,\n",
    "                         n_attempts=1e4,\n",
    "                         \n",
    "                         # For non-localizer:\n",
    "                         n_null_trials=0,\n",
    "                         response_modality='hand',\n",
    "                         \n",
    "                         # for localizer:\n",
    "                         n_localizer_blocks=None, localizer_order=None):\n",
    "    \n",
    "    \"\"\" Performs a brute force search of the best possible trial order & jitter times for a single run \"\"\"\n",
    "    n_attempts = int(n_attempts)\n",
    "    \n",
    "    # Generate n_attempt seeds to check\n",
    "    seeds = np.round(np.random.uniform(low=int(0), high=int(2**32 - 1), size=n_attempts)).astype(int)\n",
    "    effs = np.zeros(shape=seeds.shape)\n",
    "    best_eff = 0\n",
    "    \n",
    "    best_block = None\n",
    "    \n",
    "    for i in range(n_attempts):\n",
    "        # Set seed\n",
    "        np.random.seed(seed=seeds[i])\n",
    "        \n",
    "        # Generate run\n",
    "        if run_type == 'localizer':\n",
    "            block_data = create_localizer(n_localizer_blocks=n_localizer_blocks, \n",
    "                                          n_trials_per_localizer_block=n_trials, \n",
    "                                          localizer_order=localizer_order, \n",
    "                                          block_number=block_number, \n",
    "                                          pseudorandomize=pseudorandomize, TR=TR)\n",
    "        elif run_type == 'cognitive':\n",
    "            block_data = create_cognitive_block(n_trials=n_trials, \n",
    "                                                block_number=block_number, \n",
    "                                                response_modality=response_modality,\n",
    "                                                n_null_trials=n_null_trials, \n",
    "                                                TR=TR)\n",
    "        elif run_type == 'limbic':\n",
    "            block_data = create_limbic_block(n_trials=n_trials, \n",
    "                                             block_number=block_number, \n",
    "                                             response_modality=response_modality,\n",
    "                                             n_null_trials=n_null_trials,\n",
    "                                             TR=TR)\n",
    "\n",
    "        # Add trial start times (relative to start of experiment)\n",
    "        block_data['trial_start_time'] = block_data['trial_duration'].shift(1).cumsum()\n",
    "        block_data.loc[0, 'trial_start_time'] = 0\n",
    "\n",
    "        # Add cue onset times (relative to start of experiment)\n",
    "        block_data['cue_onset_time'] = block_data['trial_start_time'] + \\\n",
    "                                       block_data['phase_1']\n",
    "\n",
    "        # Add stimulus onset times (relative to start of experiment)\n",
    "        block_data['stimulus_onset_time'] = block_data['trial_start_time'] + \\\n",
    "                                            block_data['phase_1'] + \\\n",
    "                                            block_data['phase_2'] + \\\n",
    "                                            block_data['phase_3']\n",
    "        \n",
    "        # Calculate efficiency\n",
    "        X, ev_names = stim_to_design(block_data, block=block_number, silent=True)\n",
    "        \n",
    "        # Loop over the contrasts\n",
    "        dvars = [(c[ii, :].dot(np.linalg.pinv(X.T.dot(X))).dot(c[ii, :].T))\n",
    "                 for ii in range(c.shape[0])]\n",
    "        eff = c.shape[0] / np.sum(dvars)\n",
    "        effs[i] = eff\n",
    "        \n",
    "        # Found a better block than anything earlier? Save this\n",
    "        if eff > best_eff:\n",
    "            best_eff = eff\n",
    "            best_seed = seeds[i]\n",
    "            best_block = block_data\n",
    "                \n",
    "    # Save everything\n",
    "    out_dict = {'seeds': seeds, \n",
    "                'efficiencies': effs, \n",
    "                'best_eff': best_eff, \n",
    "                'best_seed': best_seed, \n",
    "                'best_block': best_block,\n",
    "                'contrasts': c,\n",
    "                'contrast_ev_names': ev_names}\n",
    "    \n",
    "    print('Done optimizing, tried %d seeds. Best efficiency: %.4f (mean eff: %.3f (SD %.3f))' % \n",
    "          (n_attempts, best_eff, np.mean(effs), np.std(effs)))\n",
    "\n",
    "    # return block\n",
    "    return best_block, out_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order of blocks by participant\n",
    "\n",
    "- X = localizer hand\n",
    "- Y = localizer eye\n",
    "- A = cognitive, hand\n",
    "- B = cognitive, eye\n",
    "- C = limbic, hand\n",
    "- D = limbic, eye\n",
    "\n",
    "4 blocks, 4\\*3\\*2\\*1 = 4! = 24 block orders\n",
    "\n",
    "2 localizer 'blocks', 2\\*1 = 2 possible orders.\n",
    "In total, 48 possible block orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('X', 'Y', 'A', 'B', 'C', 'D'),\n",
      " ('X', 'Y', 'A', 'B', 'D', 'C'),\n",
      " ('X', 'Y', 'A', 'C', 'B', 'D'),\n",
      " ('X', 'Y', 'A', 'C', 'D', 'B'),\n",
      " ('X', 'Y', 'A', 'D', 'B', 'C'),\n",
      " ('X', 'Y', 'A', 'D', 'C', 'B'),\n",
      " ('X', 'Y', 'B', 'A', 'C', 'D'),\n",
      " ('X', 'Y', 'B', 'A', 'D', 'C'),\n",
      " ('X', 'Y', 'B', 'C', 'A', 'D'),\n",
      " ('X', 'Y', 'B', 'C', 'D', 'A'),\n",
      " ('X', 'Y', 'B', 'D', 'A', 'C'),\n",
      " ('X', 'Y', 'B', 'D', 'C', 'A'),\n",
      " ('X', 'Y', 'C', 'A', 'B', 'D'),\n",
      " ('X', 'Y', 'C', 'A', 'D', 'B'),\n",
      " ('X', 'Y', 'C', 'B', 'A', 'D'),\n",
      " ('X', 'Y', 'C', 'B', 'D', 'A'),\n",
      " ('X', 'Y', 'C', 'D', 'A', 'B'),\n",
      " ('X', 'Y', 'C', 'D', 'B', 'A'),\n",
      " ('X', 'Y', 'D', 'A', 'B', 'C'),\n",
      " ('X', 'Y', 'D', 'A', 'C', 'B'),\n",
      " ('X', 'Y', 'D', 'B', 'A', 'C'),\n",
      " ('X', 'Y', 'D', 'B', 'C', 'A'),\n",
      " ('X', 'Y', 'D', 'C', 'A', 'B'),\n",
      " ('X', 'Y', 'D', 'C', 'B', 'A'),\n",
      " ('Y', 'X', 'A', 'B', 'C', 'D'),\n",
      " ('Y', 'X', 'A', 'B', 'D', 'C'),\n",
      " ('Y', 'X', 'A', 'C', 'B', 'D'),\n",
      " ('Y', 'X', 'A', 'C', 'D', 'B'),\n",
      " ('Y', 'X', 'A', 'D', 'B', 'C'),\n",
      " ('Y', 'X', 'A', 'D', 'C', 'B'),\n",
      " ('Y', 'X', 'B', 'A', 'C', 'D'),\n",
      " ('Y', 'X', 'B', 'A', 'D', 'C'),\n",
      " ('Y', 'X', 'B', 'C', 'A', 'D'),\n",
      " ('Y', 'X', 'B', 'C', 'D', 'A'),\n",
      " ('Y', 'X', 'B', 'D', 'A', 'C'),\n",
      " ('Y', 'X', 'B', 'D', 'C', 'A'),\n",
      " ('Y', 'X', 'C', 'A', 'B', 'D'),\n",
      " ('Y', 'X', 'C', 'A', 'D', 'B'),\n",
      " ('Y', 'X', 'C', 'B', 'A', 'D'),\n",
      " ('Y', 'X', 'C', 'B', 'D', 'A'),\n",
      " ('Y', 'X', 'C', 'D', 'A', 'B'),\n",
      " ('Y', 'X', 'C', 'D', 'B', 'A'),\n",
      " ('Y', 'X', 'D', 'A', 'B', 'C'),\n",
      " ('Y', 'X', 'D', 'A', 'C', 'B'),\n",
      " ('Y', 'X', 'D', 'B', 'A', 'C'),\n",
      " ('Y', 'X', 'D', 'B', 'C', 'A'),\n",
      " ('Y', 'X', 'D', 'C', 'A', 'B'),\n",
      " ('Y', 'X', 'D', 'C', 'B', 'A')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "from pprint import pprint\n",
    "\n",
    "# Use itertools permutations to get all possible orders of both blocks and localizers\n",
    "block_order = list(itertools.permutations(\"ABCD\"))\n",
    "loc_order = list(itertools.permutations(\"XY\"))\n",
    "\n",
    "# Repeat all elements in loc_orders to match sizes\n",
    "loc_order = [item for item in loc_order for i in range(len(block_order))]\n",
    "\n",
    "# Merge localizer and blocks\n",
    "block_order = [(x[0], x[1], y[0], y[1], y[2], y[3]) for x, y in zip(loc_order, block_order*2)]\n",
    "pprint(block_order)\n",
    "len(block_order)  # 48 possible conditions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over participant numbers to generate the correct blocks in order, and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 12, 24, 36, 48, 60, 72, 84, 96, 108]\n"
     ]
    }
   ],
   "source": [
    "a = [x for x in range(120) if x % 4 == 0 and x % 6 == 0]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('/Users/steven/Sync/PhDprojects/subcortex/flashtask/designs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pp 001...\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 132.7095 (mean eff: 88.095 (SD 13.515))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 6604.0248 (mean eff: 5836.669 (SD 253.697))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 6724.0523 (mean eff: 5837.793 (SD 256.391))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 1721.1368 (mean eff: 1499.501 (SD 62.665))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 1708.8358 (mean eff: 1504.463 (SD 64.408))\n",
      "Processing pp 002...\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 158.1969 (mean eff: 88.361 (SD 13.256))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 6684.9555 (mean eff: 5829.290 (SD 255.856))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 6722.7564 (mean eff: 5841.919 (SD 258.664))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 1689.2522 (mean eff: 1504.387 (SD 61.640))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 1712.6617 (mean eff: 1503.821 (SD 63.832))\n",
      "Processing pp 003...\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 167.6944 (mean eff: 88.736 (SD 13.129))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 6850.0763 (mean eff: 5831.521 (SD 251.200))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 1675.7628 (mean eff: 1504.452 (SD 61.143))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 6896.6558 (mean eff: 5830.344 (SD 254.349))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 1679.6420 (mean eff: 1505.858 (SD 61.622))\n",
      "Processing pp 004...\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 150.9995 (mean eff: 89.205 (SD 14.000))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 6590.1333 (mean eff: 5823.044 (SD 245.026))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 1721.9112 (mean eff: 1505.158 (SD 62.365))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 1694.5066 (mean eff: 1503.770 (SD 62.376))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 6540.1426 (mean eff: 5826.022 (SD 246.940))\n",
      "Processing pp 005...\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 147.5036 (mean eff: 87.652 (SD 13.194))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 6710.7629 (mean eff: 5824.229 (SD 256.211))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 1730.7837 (mean eff: 1510.088 (SD 63.275))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 6653.4482 (mean eff: 5838.649 (SD 255.078))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 1688.0243 (mean eff: 1503.379 (SD 63.335))\n",
      "Processing pp 006...\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 153.1466 (mean eff: 88.611 (SD 13.433))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 6481.1965 (mean eff: 5842.754 (SD 249.933))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 1722.4079 (mean eff: 1504.580 (SD 63.455))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 1668.6725 (mean eff: 1506.423 (SD 62.233))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 6637.5308 (mean eff: 5831.598 (SD 256.859))\n",
      "Processing pp 007...\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 132.1724 (mean eff: 88.719 (SD 13.688))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 6726.1995 (mean eff: 5836.029 (SD 252.449))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 6673.8891 (mean eff: 5843.264 (SD 255.542))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 1776.8346 (mean eff: 1505.702 (SD 63.101))\n",
      "Done optimizing, tried 1000 seeds. Best efficiency: 1717.1461 (mean eff: 1506.807 (SD 63.573))\n",
      "Processing pp 008...\n"
     ]
    }
   ],
   "source": [
    "n_trials_per_localizer_block = 8\n",
    "n_localizer_blocks = 6\n",
    "TR = 3\n",
    "\n",
    "os.chdir('/Users/steven/Sync/PhDprojects/subcortex/flashtask/designs')\n",
    "\n",
    "n_optim_attempts = 1e3\n",
    "n_trials_limbic = 84\n",
    "n_trials_cognitive = 72\n",
    "n_null_trials_limbic = 8\n",
    "n_null_trials_cognitive = 7\n",
    "n_participants = 100\n",
    "\n",
    "## Contrasts to optim for\n",
    "c_localizer = np.array([\n",
    "        [0, 0, 0, 0, 1, 1, 0, 0],  # hand vs baseline\n",
    "        [0, 0, 0, 0, 0, 0, 1, 1],  # eye vs baseline\n",
    "        [0, 0, 0, 0, 1, 1, -1, -1] # hand - eye\n",
    "    ])\n",
    "\n",
    "# Cognitive: [u'cue_ACC', u'cue_SPD', u'stimulus_0_ACC', u'stimulus_0_SPD', u'stimulus_1_ACC', u'stimulus_1_SPD']\n",
    "# Of interest: ACC vs SPD cue\n",
    "# ACC vs SPD stimulus\n",
    "# ACC vs SPD trial (cue + stim)\n",
    "c_cognitive = np.array([\n",
    "    [1, -1, 0, 0, 0, 0],  # ACC vs SPD cue\n",
    "    [0, 0, 1, -1, 1, -1],  # ACC vs SPD stimulus\n",
    "    [1, -1, 1, -1, 1, -1]  # ACC vs SPD cue and stimulus\n",
    "])\n",
    "\n",
    "# Limbic:\n",
    "# [u'cue_RIGHT', u'cue_NEU', u'cue_LEFT', \n",
    "# u'stimulus_1_RIGHT', u'stimulus_1_NEU', u'stimulus_1_LEFT', \n",
    "# u'stimulus_0_RIGHT', u'stimulus_0_NEU', u'stimulus_0_LEFT']\n",
    "# Of interest: direction vs neutral cue; direction vs neutral stim; direction vs neutral cue+stim\n",
    "c_limbic = np.array([\n",
    "    [1, -2, 1, 0, 0, 0, 0, 0, 0],  # direction vs neutral cue\n",
    "    [0, 0, 0, 1, -2, 1, 1, -2, 1], # direction vs neutral stim\n",
    "    [1, -2, 1, 1, -2, 1, 1, -2, 1] # direction vs neutral cue+stim\n",
    "])\n",
    "\n",
    "# Loop & Run\n",
    "for pp in range(0, n_participants):\n",
    "    pp_str = str(pp+1).zfill(3)\n",
    "    print('Processing pp %s...' % pp_str)\n",
    "    block_order_this_pp = block_order[pp % len(block_order)]\n",
    "    \n",
    "    # Empty DataFrame\n",
    "    design_this_pp = pd.DataFrame()\n",
    "    \n",
    "    # Get localizer block\n",
    "    if block_order_this_pp[0] == 'X':\n",
    "        localizer_order = ['hand', 'eye']\n",
    "    else:\n",
    "        localizer_order = ['eye', 'hand']\n",
    "\n",
    "    design_this_pp, optim_res = optimize_brute_force(n_trials=n_trials_per_localizer_block,\n",
    "                                                     c=c_localizer, \n",
    "                                                     run_type='localizer',\n",
    "                                                     block_number=0,\n",
    "                                                     pseudorandomize=True, \n",
    "                                                     TR=3,\n",
    "                                                     n_attempts=n_optim_attempts,\n",
    "                                                     n_localizer_blocks=6, \n",
    "                                                     localizer_order=localizer_order)\n",
    "    with open('pp_' + pp_str + '_block_0_optim.pkl', 'wb') as f:\n",
    "        pkl.dump(optim_res, f)\n",
    "    \n",
    "    # Get blocks\n",
    "    for block_number, block_char in enumerate(block_order_this_pp):\n",
    "        if block_char in ['X', 'Y']:\n",
    "            continue\n",
    "        \n",
    "        if block_char == 'A':\n",
    "            block_data, optim_res = optimize_brute_force(n_trials=n_trials_cognitive, \n",
    "                                                         c=c_cognitive,\n",
    "                                                         run_type='cognitive',\n",
    "                                                         block_number=block_number-1, \n",
    "                                                         response_modality='hand',\n",
    "                                                         n_null_trials=n_null_trials_cognitive, \n",
    "                                                         TR=TR,\n",
    "                                                         pseudorandomize=True,\n",
    "                                                         n_attempts=n_optim_attempts)\n",
    "        elif block_char == 'B':\n",
    "            block_data, optim_res = optimize_brute_force(n_trials=n_trials_cognitive, \n",
    "                                                         c=c_cognitive,\n",
    "                                                         run_type='cognitive',\n",
    "                                                         block_number=block_number-1, \n",
    "                                                         response_modality='eye',\n",
    "                                                         n_null_trials=n_null_trials_cognitive, \n",
    "                                                         TR=TR,\n",
    "                                                         pseudorandomize=True,\n",
    "                                                         n_attempts=n_optim_attempts)\n",
    "        elif block_char == 'C':\n",
    "            block_data, optim_res = optimize_brute_force(n_trials=n_trials_limbic, \n",
    "                                                         c=c_limbic,\n",
    "                                                         run_type='limbic',\n",
    "                                                         block_number=block_number-1, \n",
    "                                                         response_modality='hand',\n",
    "                                                         n_null_trials=n_null_trials_limbic, \n",
    "                                                         TR=TR,\n",
    "                                                         pseudorandomize=True,\n",
    "                                                         n_attempts=n_optim_attempts)\n",
    "        elif block_char == 'D':\n",
    "            block_data, optim_res = optimize_brute_force(n_trials=n_trials_limbic, \n",
    "                                                         c=c_limbic,\n",
    "                                                         run_type='limbic',\n",
    "                                                         block_number=block_number-1, \n",
    "                                                         response_modality='eye',\n",
    "                                                         n_null_trials=n_null_trials_limbic, \n",
    "                                                         TR=TR,\n",
    "                                                         pseudorandomize=True,\n",
    "                                                         n_attempts=n_optim_attempts)\n",
    "\n",
    "        with open('pp_' + pp_str + '_block_' + str(block_number) + '_optim.pkl', 'wb') as f:\n",
    "            pkl.dump(optim_res, f)\n",
    "        design_this_pp = design_this_pp.append(block_data)\n",
    "\n",
    "    # Set indices\n",
    "    design_this_pp.index.name = 'block_trial_ID'\n",
    "    design_this_pp.reset_index(inplace=True)\n",
    "    design_this_pp.index.name = 'trial_ID'\n",
    "    \n",
    "    # Add trial start times (relative to start of experiment)\n",
    "    design_this_pp['trial_start_time'] = design_this_pp['trial_duration'].shift(1).cumsum()\n",
    "    design_this_pp.loc[0, 'trial_start_time'] = 0\n",
    "\n",
    "    # Add cue onset times (relative to start of experiment)\n",
    "    design_this_pp['cue_onset_time'] = design_this_pp['trial_start_time'] + \\\n",
    "                                       design_this_pp['phase_1']\n",
    "\n",
    "    # Add stimulus onset times (relative to start of experiment)\n",
    "    design_this_pp['stimulus_onset_time'] = design_this_pp['trial_start_time'] + \\\n",
    "                                            design_this_pp['phase_1'] + \\\n",
    "                                            design_this_pp['phase_2'] + \\\n",
    "                                            design_this_pp['phase_3']\n",
    "    \n",
    "    # Re-order column order for nicety\n",
    "    design_this_pp = design_this_pp[['block_trial_ID', 'block', 'block_type', 'null_trial', 'correct_answer', 'cue', 'response_modality', 'trial_type', \n",
    "                                     'phase_0', 'phase_1', 'phase_2', 'phase_3', 'phase_4', 'phase_5', 'phase_6', 'phase_7', 'trial_duration', \n",
    "                                     'trial_start_time', 'cue_onset_time', 'stimulus_onset_time',\n",
    "                                     'trial_start_time_block', 'cue_onset_time_block', 'stimulus_onset_time_block']]\n",
    "    \n",
    "    # Save full data\n",
    "    if not os.path.exists(os.path.join('pp_%s' % pp_str, 'all_blocks')):\n",
    "        os.makedirs(os.path.join('pp_%s' % pp_str, 'all_blocks'))\n",
    "    design_this_pp.to_csv(os.path.join('pp_%s' % pp_str, 'all_blocks', 'trials.csv'), index=True)\n",
    "    \n",
    "    # Save individual blocks\n",
    "    for block_num, block_type in zip(design_this_pp['block'].unique(), design_this_pp['block_type'].unique()):\n",
    "        block = design_this_pp.loc[design_this_pp['block'] == block_num]\n",
    "        \n",
    "        if not os.path.exists(os.path.join('pp_%s' % pp_str, 'block_%d_type_%s' % (block_num, block_type))):\n",
    "            os.makedirs(os.path.join('pp_%s' % pp_str, 'block_%d_type_%s' % (block_num, block_type)))\n",
    "        \n",
    "        block.to_csv(os.path.join('pp_%s' % pp_str, 'block_%d_type_%s' % (block_num, block_type), 'trials.csv'), index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating EVs, .fsf-files, and running FSL\n",
    "All following code is used to transform the created designs into FSL-accepted formats. First, we create 3-column .txt-files for every regressor. Then, we *manually* and painfully make the .fsf file for a single subject (for all designs) in the FSL gui (if someone can point me to a CLI for this, I'd be very grateful).\n",
    "Finally, we copy the created .fsf-file and substitute all references to the first pp for each other pp.\n",
    "\n",
    "The current set-up is to model the following EVs:\n",
    "- Localizer (8 EVs):\n",
    " 1. cue: direction x response modality (left/right x eye/hand) = 4 EVs\n",
    " 2. response: direction x response modality (left/right x eye/hand) = 4 EVs.\n",
    "- Limbic blocks (9 EVs):\n",
    " 1. cue: direction (left/right/neutral) = 3 EVs\n",
    " 2. stimulus: cued direction (left/right/neutral) x stimulus direction (left/right) = 6 EVs\n",
    "- Cognitive blocks (6 EVs):\n",
    " 1. cue: instruction (spd/acc) = 2 EVs\n",
    " 2. stimulus: cued instruction (spd/acc) x stimulus direction (left/right) = 4 EVs\n",
    "\n",
    "In total, the first-level design has 23 EVs (currently not including responses). In the following code, we extract the EV timing from the design files.\n",
    "\n",
    "### Create EV .txts files that can be read by FSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing pp 1...\n",
      "Processing pp 2...\n",
      "Processing pp 3...\n",
      "Processing pp 4...\n",
      "Processing pp 5...\n",
      "Processing pp 6...\n",
      "Processing pp 7...\n",
      "Processing pp 8...\n",
      "Processing pp 9...\n",
      "Processing pp 10...\n",
      "Processing pp 11...\n",
      "Processing pp 12...\n"
     ]
    }
   ],
   "source": [
    "for pp_num in range(0, 12):\n",
    "    print('Processing pp %d...' % (pp_num+1))\n",
    "    \n",
    "    pp_str = str(pp_num+1).zfill(3)\n",
    "    \n",
    "    # Get all blocks directories of this subject, making sure to ignore any existing .feat-dirs\n",
    "    pp_block_dirs = glob('pp_%s/*' % pp_str)\n",
    "    pp_block_dirs = [x for x in pp_block_dirs if not x.endswith('.feat')]\n",
    "    \n",
    "    # Loop over blocks\n",
    "    for pp_block_dir in pp_block_dirs:\n",
    "        \n",
    "        # For all blocks, or the localizer block, we can use the \"global\" timing (not within-block) to create EVs.\n",
    "        if 'all_blocks' in pp_block_dir or 'localizer' in pp_block_dir:\n",
    "            stim_onset_col = 'stimulus_onset_time'\n",
    "            cue_onset_col = 'cue_onset_time'\n",
    "        else:  # Otherwise, use only the block timing\n",
    "            stim_onset_col = 'stimulus_onset_time_block'\n",
    "            cue_onset_col = 'cue_onset_time_block'\n",
    "        \n",
    "        # Define output directory, create if it doesnt exist\n",
    "        output_dir = os.path.join(pp_block_dir, 'evs')\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # Read design from csv\n",
    "        design = pd.read_csv(os.path.join(pp_block_dir, 'trials.csv'))\n",
    "        design = design[['correct_answer','trial_type', 'block', 'phase_2', 'phase_4', 'cue', cue_onset_col, \n",
    "                         stim_onset_col, 'response_modality', 'null_trial']]\n",
    "        \n",
    "        # Get rid of all null trials\n",
    "        design = design.loc[design['null_trial'] != True]\n",
    "        \n",
    "        # Weights of all EVs are 1\n",
    "        design['weight'] = 1\n",
    "        \n",
    "        # For the localizer block\n",
    "        if 0 in design['block'].unique():\n",
    "            subs = design.loc[design['block'] == 0]\n",
    "            \n",
    "            # EVs for cues and responses, separated for the response modality (eye/hand) and direction (left/right): \n",
    "            # 4 EVs for cue (left/right x eye/hand), and 4 EVs for the responses (left/right x eye/hand)\n",
    "            for effector in subs['response_modality'].unique():\n",
    "                for cue in subs['cue'].unique():\n",
    "                    # response\n",
    "                    ev = subs.loc[(subs['response_modality'] == effector) & \n",
    "                                  (subs['cue'] == cue), [stim_onset_col, 'phase_4', 'weight']].values.tolist()\n",
    "                    \n",
    "                    with open(os.path.join(output_dir, 'ev_resp_%s_%s.txt' % (effector, cue)), 'wb') as f:\n",
    "                        for _list in ev:\n",
    "                            for _string in _list:\n",
    "                                f.write(str(_string) + '\\n')\n",
    "                            f.write('\\n')\n",
    "                    \n",
    "                    # responses\n",
    "                    ev = subs.loc[(subs['response_modality'] == effector) & \n",
    "                                  (subs['cue'] == cue), [cue_onset_col, 'phase_2', 'weight']].values.tolist()\n",
    "                    \n",
    "                    with open(os.path.join(output_dir, 'ev_cue_%s_%s.txt' % (effector, cue)), 'wb') as f:\n",
    "                        for _list in ev:\n",
    "                            for _string in _list:\n",
    "                                f.write(str(_string) + '\\n')\n",
    "                            f.write('\\n')\n",
    "                              \n",
    "            # Get rid of localizer block here\n",
    "            design = design.loc[design['block'] > 0]\n",
    "        \n",
    "        # For all decision-making blocks, we model the cue types (spd/acc or left/neu/right), so create EVs for these\n",
    "        for cue_type in design['cue'].unique():\n",
    "            \n",
    "            # Get cue onset time, cue duration, and cue weight\n",
    "            ev = design.loc[(design['cue'] == cue_type), [cue_onset_col, 'phase_2', 'weight']].values.tolist()\n",
    "\n",
    "            with open(os.path.join(output_dir, 'ev_cue_%s.txt' % cue_type), 'wb') as f:\n",
    "                for _list in ev:\n",
    "                    for _string in _list:\n",
    "                        f.write(str(_string) + '\\n')\n",
    "                    f.write('\\n')\n",
    "\n",
    "        # We also model the stimuli, separate for type (left/right), but separate for every cue type (spd/acc OR left/neu/right)\n",
    "        for stim_type in design['correct_answer'].unique():\n",
    "            if np.isnan(stim_type):  # a nan stimtype corresponds to a null trial, so skip these\n",
    "                continue\n",
    "\n",
    "            for cue_type in design['cue'].unique():\n",
    "                \n",
    "                # Get stimulus onset time, stimulus duration, and weight\n",
    "                ev = design.loc[(design['correct_answer'] == stim_type) &\n",
    "                                (design['cue'] == cue_type), \n",
    "                                [stim_onset_col, 'phase_4', 'weight']].values.tolist()\n",
    "\n",
    "                with open(os.path.join(output_dir, 'ev_stimulus_%d_%s.txt' % (stim_type, cue_type)), 'wb') as f:\n",
    "                    for _list in ev:\n",
    "                        for _string in _list:\n",
    "                            f.write(str(_string) + '\\n')\n",
    "                        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load FSL design text file for pp 1, and create for all other pps\n",
    "Before running this, the .fsf-design files for pp1 (each block) should be created manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_pp = 12\n",
    "for block_name in ['all_blocks', '_type_localizer', '_type_cognitive_hand', '_type_cognitive_eye', '_type_limbic_eye', '*_type_limbic_hand']:\n",
    "    # Get .fsf-file from pp001\n",
    "    pp_001_fn = glob(os.path.join('pp_001', '*' + block_name, 'design.fsf'))[0]\n",
    "    pp_001_block_name = pp_001_fn.split('/')[1]\n",
    "    \n",
    "    with open(pp_001_fn, 'rb') as f:\n",
    "        fsf_templ = f.readlines()\n",
    "\n",
    "    for pp in range(1, max_pp+1):\n",
    "        fsf_thispp = copy.copy(fsf_templ)\n",
    "        \n",
    "        # Get path to save the design.fsf file for this pp\n",
    "        this_pp_fn = os.path.join(glob(os.path.join('pp_' + str(pp).zfill(3), '*' + block_name))[0], 'design.fsf')\n",
    "        this_pp_block_name = this_pp_fn.split('/')[1]\n",
    "        \n",
    "        for i, line in enumerate(fsf_thispp):\n",
    "            if not block_name == 'all_blocks':\n",
    "                fsf_thispp[i] = re.sub(pp_001_block_name,\n",
    "                                       this_pp_block_name, line)\n",
    "            fsf_thispp[i] = re.sub('pp_001', 'pp_' + str(pp).zfill(3), fsf_thispp[i])\n",
    "\n",
    "        # Find fn for current pp\n",
    "        with open(this_pp_fn, 'wb') as f:\n",
    "            f.writelines(fsf_thispp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop over subject and block directories, calling command line feat every time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wd = os.getcwd()\n",
    "for sub in range(1, 12+1):\n",
    "    os.chdir(wd)\n",
    "    sub_dir = 'pp_' + str(sub).zfill(3)\n",
    "    block_dirs = [f for f in os.listdir(sub_dir) if os.path.isdir(os.path.join(sub_dir, f))]\n",
    "    block_dirs = [f for f in block_dirs if not '.feat' in f]\n",
    "    \n",
    "    for block_dir in block_dirs:\n",
    "        design_dir = os.path.join(wd, sub_dir, block_dir)\n",
    "        os.chdir(design_dir)\n",
    "        os.system('feat design.fsf')\n",
    "\n",
    "os.chdir(wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## The following is old, do not run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "design creation without optimizing:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_trials_per_localizer_block = 8\n",
    "n_localizer_blocks = 6\n",
    "TR = 3\n",
    "\n",
    "os.chdir('/Users/steven/Sync/PhDprojects/subcortex/flashtask/designs')\n",
    "\n",
    "n_trials_limbic = 84\n",
    "n_trials_cognitive = 72\n",
    "n_null_trials_limbic = 8\n",
    "n_null_trials_cognitive = 7\n",
    "n_participants = 100\n",
    "\n",
    "for pp in range(0, n_participants):\n",
    "    pp_str = str(pp+1).zfill(3)\n",
    "    block_order_this_pp = block_order[pp % len(block_order)]\n",
    "    \n",
    "    # Empty DataFrame\n",
    "    design_this_pp = pd.DataFrame()\n",
    "    \n",
    "    # Get localizer block\n",
    "    if block_order_this_pp[0] == 'X':\n",
    "        localizer_order = ['hand', 'eye']\n",
    "    else:\n",
    "        localizer_order = ['eye', 'hand']\n",
    "\n",
    "    for localizer_block in range(int(n_localizer_blocks/2)):\n",
    "        block_data = pd.DataFrame()\n",
    "        for localizer_block in range(int(n_localizer_blocks/2)):\n",
    "            loc_block = create_localizer_block_single_effector(n_trials=n_trials_per_localizer_block, \n",
    "                                                response_modality=localizer_order[0],\n",
    "                                                block_number=0,\n",
    "                                                pseudorandomize=True, TR=TR)\n",
    "            block_data = block_data.append(loc_block)\n",
    "            loc_block = create_localizer_block_single_effector(n_trials=n_trials_per_localizer_block, \n",
    "                                                response_modality=localizer_order[1],\n",
    "                                                block_number=0,\n",
    "                                                pseudorandomize=True, TR=TR)\n",
    "            block_data = block_data.append(loc_block)\n",
    "    design_this_pp = design_this_pp.append(block_data)\n",
    "\n",
    "    # Get blocks\n",
    "    for block_number, block_char in enumerate(block_order_this_pp):\n",
    "        if block_char in ['X', 'Y']:\n",
    "            continue\n",
    "        \n",
    "        if block_char == 'A':\n",
    "            block_data = create_cognitive_block(n_trials=n_trials_cognitive, \n",
    "                                                block_number=block_number-1, \n",
    "                                                response_modality='hand',\n",
    "                                                n_null_trials=n_null_trials_cognitive, \n",
    "                                                TR=TR)\n",
    "        elif block_char == 'B':\n",
    "            block_data = create_cognitive_block(n_trials=n_trials_cognitive, \n",
    "                                                block_number=block_number-1, \n",
    "                                                response_modality='eye',\n",
    "                                                n_null_trials=n_null_trials_cognitive,\n",
    "                                                TR=TR)\n",
    "        elif block_char == 'C':\n",
    "            block_data = create_limbic_block(n_trials=n_trials_limbic, \n",
    "                                             block_number=block_number-1, \n",
    "                                             response_modality='hand',\n",
    "                                             n_null_trials=n_null_trials_limbic,\n",
    "                                             TR=TR)\n",
    "        elif block_char == 'D':\n",
    "            block_data = create_limbic_block(n_trials=n_trials_limbic, \n",
    "                                             block_number=block_number-1, \n",
    "                                             response_modality='eye',\n",
    "                                             n_null_trials=n_null_trials_limbic,\n",
    "                                             TR=TR)\n",
    "        \n",
    "        design_this_pp = design_this_pp.append(block_data)\n",
    "\n",
    "    # Set indices\n",
    "    design_this_pp.index.name = 'block_trial_ID'\n",
    "    design_this_pp.reset_index(inplace=True)\n",
    "    design_this_pp.index.name = 'trial_ID'\n",
    "    \n",
    "    # Add trial start times (relative to start of experiment)\n",
    "    design_this_pp['trial_start_time'] = design_this_pp['trial_duration'].shift(1).cumsum()\n",
    "    design_this_pp.loc[0, 'trial_start_time'] = 0\n",
    "\n",
    "    # Add cue onset times (relative to start of experiment)\n",
    "    design_this_pp['cue_onset_time'] = design_this_pp['trial_start_time'] + \\\n",
    "                                       design_this_pp['phase_1']\n",
    "\n",
    "    # Add stimulus onset times (relative to start of experiment)\n",
    "    design_this_pp['stimulus_onset_time'] = design_this_pp['trial_start_time'] + \\\n",
    "                                            design_this_pp['phase_1'] + \\\n",
    "                                            design_this_pp['phase_2'] + \\\n",
    "                                            design_this_pp['phase_3']\n",
    "    \n",
    "    # Re-order column order for nicety\n",
    "    design_this_pp = design_this_pp[['block_trial_ID', 'block', 'block_type', 'null_trial', 'correct_answer', 'cue', 'response_modality', 'trial_type', \n",
    "                                     'phase_0', 'phase_1', 'phase_2', 'phase_3', 'phase_4', 'phase_5', 'phase_6', 'phase_7', 'trial_duration', \n",
    "                                     'trial_start_time', 'cue_onset_time', 'stimulus_onset_time',\n",
    "                                     'trial_start_time_block', 'cue_onset_time_block', 'stimulus_onset_time_block']]\n",
    "    \n",
    "    # Save full data\n",
    "    if not os.path.exists(os.path.join('pp_%s' % pp_str, 'all_blocks')):\n",
    "        os.makedirs(os.path.join('pp_%s' % pp_str, 'all_blocks'))\n",
    "    design_this_pp.to_csv(os.path.join('pp_%s' % pp_str, 'all_blocks', 'trials.csv'), index=True)\n",
    "    \n",
    "    # Save individual blocks\n",
    "    for block_num, block_type in zip(design_this_pp['block'].unique(), design_this_pp['block_type'].unique()):\n",
    "        block = design_this_pp.loc[design_this_pp['block'] == block_num]\n",
    "        \n",
    "        if not os.path.exists(os.path.join('pp_%s' % pp_str, 'block_%d_type_%s' % (block_num, block_type))):\n",
    "            os.makedirs(os.path.join('pp_%s' % pp_str, 'block_%d_type_%s' % (block_num, block_type)))\n",
    "        \n",
    "        block.to_csv(os.path.join('pp_%s' % pp_str, 'block_%d_type_%s' % (block_num, block_type), 'trials.csv'), index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def create_localizer_block_single_effector_type2(n_trials, response_modality='eye', \n",
    "                                                 block_number=None, pseudorandomize=True, \n",
    "                                                 add_timing=True, null_trials=0, TR=2):\n",
    "    \"\"\"\n",
    "    This is NOT used\n",
    "    \"\"\"\n",
    "    # Only two trial types here: 'left' is correct, or 'right' is correct\n",
    "    trial_types = np.hstack((np.repeat([0, 1], repeats=n_trials / 4),   # Pro-saccadic cue, left/right corr\n",
    "                             np.repeat([2, 3], repeats=n_trials / 4)))  # Anti-saccadic cue, left/right corr\n",
    "    \n",
    "    # Initialize arrays\n",
    "    cue_by_trial = np.zeros(n_trials, dtype='<U10')\n",
    "    correct_answers = np.zeros(n_trials, dtype=np.int8)\n",
    "\n",
    "    # Define the cues for every trial\n",
    "    cue_by_trial[(trial_types == 0)] = 'pro_LEFT'\n",
    "    cue_by_trial[(trial_types == 1)] = 'pro_RIGHT'\n",
    "    cue_by_trial[(trial_types == 2)] = 'anti_LEFT'\n",
    "    cue_by_trial[(trial_types == 3)] = 'anti_RIGHT'\n",
    "\n",
    "    # Define the responses ('correct answers')/directions for every trial\n",
    "    correct_answers[(trial_types == 0) | (trial_types == 2)] = 0  # 0 = respond LEFT\n",
    "    correct_answers[(trial_types == 1) | (trial_types == 3)] = 1  # 1 = respond RIGHT\n",
    "    \n",
    "    # Create dataframe for easier handling\n",
    "    trial_data = pd.DataFrame({'correct_answer': correct_answers,\n",
    "                               'cue': cue_by_trial,\n",
    "                               'trial_type': trial_types})\n",
    "    \n",
    "    # Should we pseudorandomize?\n",
    "    if pseudorandomize:\n",
    "        trial_data = Pseudorandomizer(trial_data, max_identical_iters={'correct_answer': 3,\n",
    "                                                                       'cue': 3}).run()\n",
    "    \n",
    "    trial_data['null_trial'] = False\n",
    "    if null_trials > 0:\n",
    "        trial_data = add_pseudorandom_null_trials(trial_data, n_null_trials=null_trials)\n",
    "    \n",
    "    # Add block number for completeness\n",
    "    if block_number is not None:\n",
    "        trial_data['block'] = block_number\n",
    "        trial_data['block_type'] = 'localizer'\n",
    "\n",
    "    # Usually, we also want to add the duration of all the 'trial phases'\n",
    "    if add_timing:\n",
    "        # Set phase_3 and phase_0 to 0 (no post-cue fixcross, no feedback)\n",
    "        trial_data = get_localizer_timing(trial_data, TR=TR)\n",
    "#         trial_data['response_duration'] = 1.5\n",
    "    \n",
    "    trial_data['response_modality'] = response_modality.lower()\n",
    "    \n",
    "    return trial_data"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
